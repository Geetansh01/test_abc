{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib as plt\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(37)\n",
    "rn.seed(1254)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, train, test, validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_data = \"./data_for_model/Sentences_420.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Shape: (420, 4)\n",
      "First 5 rows with the new 'InputText' column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_CONTEXT</th>\n",
       "      <th>URL_ROOT</th>\n",
       "      <th>Label</th>\n",
       "      <th>InputText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>developer</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>1</td>\n",
       "      <td>USER_CONTEXT: \"developer\"; URL_ROOT: \"stackove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marketing</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>1</td>\n",
       "      <td>USER_CONTEXT: \"marketing\"; URL_ROOT: \"stackove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human_resource</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>1</td>\n",
       "      <td>USER_CONTEXT: \"human_resource\"; URL_ROOT: \"sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sales</td>\n",
       "      <td>stackoverflow</td>\n",
       "      <td>1</td>\n",
       "      <td>USER_CONTEXT: \"sales\"; URL_ROOT: \"stackoverflow\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>developer</td>\n",
       "      <td>github</td>\n",
       "      <td>1</td>\n",
       "      <td>USER_CONTEXT: \"developer\"; URL_ROOT: \"github\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         USER_CONTEXT       URL_ROOT  Label  \\\n",
       "S.No.                                         \n",
       "1           developer  stackoverflow      1   \n",
       "2           marketing  stackoverflow      1   \n",
       "3      human_resource  stackoverflow      1   \n",
       "4               sales  stackoverflow      1   \n",
       "5           developer         github      1   \n",
       "\n",
       "                                               InputText  \n",
       "S.No.                                                     \n",
       "1      USER_CONTEXT: \"developer\"; URL_ROOT: \"stackove...  \n",
       "2      USER_CONTEXT: \"marketing\"; URL_ROOT: \"stackove...  \n",
       "3      USER_CONTEXT: \"human_resource\"; URL_ROOT: \"sta...  \n",
       "4       USER_CONTEXT: \"sales\"; URL_ROOT: \"stackoverflow\"  \n",
       "5          USER_CONTEXT: \"developer\"; URL_ROOT: \"github\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>420.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.920526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Label\n",
       "count  420.000000\n",
       "mean     0.047619\n",
       "std      0.920526\n",
       "min     -1.000000\n",
       "25%     -1.000000\n",
       "50%      0.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDA\n",
    "# Load data and create the combined input column \n",
    "path_to_data = \"./data_for_model/Sentences_420.csv\"\n",
    "df = pd.read_csv(path_to_data, index_col='S.No.')\n",
    "\n",
    "# Programmatically create the input string required by the model\n",
    "# Format: USER_CONTEXT: \"value\"; URL_ROOT: \"value\"\n",
    "df['InputText'] = df.apply(\n",
    "    lambda row: f\"USER_CONTEXT: \\\"{row['USER_CONTEXT']}\\\"; URL_ROOT: \\\"{row['URL_ROOT']}\\\"\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Data loaded successfully. Shape:\", df.shape)\n",
    "print(\"First 5 rows with the new 'InputText' column:\")\n",
    "display(df.head())\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Geetansh\\Desktop\\URLClassification\\Mlvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits created:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['InputText', 'Label', 'S.No.'],\n",
       "    num_rows: 336\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['InputText', 'Label', 'S.No.'],\n",
       "    num_rows: 42\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['InputText', 'Label', 'S.No.'],\n",
       "    num_rows: 42\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make test, train, cv splits using percentages\n",
    "from datasets import Dataset\n",
    "\n",
    "# The old code used a 'Sentence' column. We select only the necessary columns for the new task.\n",
    "ds = Dataset.from_pandas(df[['InputText', 'Label']])\n",
    "\n",
    "# Split: 80% for training, 20% for temp (test + validation)\n",
    "ds_train_temp_dict = ds.train_test_split(test_size=0.2, seed=42)\n",
    "ds_train = ds_train_temp_dict['train']\n",
    "\n",
    "# Split the 20% temp set into half for validation (10%) and test (10%)\n",
    "ds_test_cv_dict = ds_train_temp_dict['test'].train_test_split(test_size=0.5, seed=42)\n",
    "ds_cv = ds_test_cv_dict['train']\n",
    "ds_test = ds_test_cv_dict['test']\n",
    "\n",
    "print(\"Dataset splits created:\")\n",
    "display(ds_train)\n",
    "display(ds_cv)\n",
    "display(ds_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Geetansh\\Desktop\\URLClassification\\Mlvenv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 336/336 [00:00<00:00, 4852.01 examples/s]\n",
      "Map: 100%|██████████| 336/336 [00:00<00:00, 9809.12 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 2592.35 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset after tokenization and casting labels to float:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['InputText', 'labels', 'S.No.', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 336\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note the data type of the 'labels' column is now float:\n",
      "{'InputText': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None), 'S.No.': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Get Tokenizer and Tokenize the 'InputText' column\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_nm = 'microsoft/deberta-v3-small'\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "\n",
    "def tokenize_dataset(ds, tokenizer):\n",
    "    '''\n",
    "    Tokenizes the 'InputText' column and casts the 'labels' column to float.\n",
    "    '''\n",
    "    # Tokenize the input text\n",
    "    tokenized_ds = ds.map(lambda row: tokenizer(row['InputText']), batched=False)\n",
    "    \n",
    "    # Rename 'Label' to 'labels' as required by the Trainer\n",
    "    if 'Label' in tokenized_ds.column_names:\n",
    "        tokenized_ds = tokenized_ds.rename_columns({'Label' : 'labels'})\n",
    "        \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Cast the 'labels' column to a float type to match model output\n",
    "    tokenized_ds = tokenized_ds.map(lambda example: {'labels': float(example['labels'])})\n",
    "    # ----------------------\n",
    "    \n",
    "    return tokenized_ds\n",
    "\n",
    "# Apply the corrected tokenization function to the train and validation splits\n",
    "tokenized_ds_train = tokenize_dataset(ds_train, tokz)\n",
    "tokenized_ds_cv = tokenize_dataset(ds_cv, tokz)\n",
    "\n",
    "print(\"Training dataset after tokenization and casting labels to float:\")\n",
    "display(tokenized_ds_train)\n",
    "print(\"Note the data type of the 'labels' column is now float:\")\n",
    "print(tokenized_ds_train.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CELL: Define a custom data collator for regression\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "class DataCollatorForRegression(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer=tokenizer)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # This is the default behavior of DataCollatorWithPadding\n",
    "        batch = super().__call__(features)\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # The default collator may leave labels as Long. We explicitly cast them to Float.\n",
    "        # This ensures the data type is correct right before it hits the model's loss function.\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float)\n",
    "        # ----------------------\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Geetansh\\Desktop\\URLClassification\\Mlvenv\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Geetansh\\AppData\\Local\\Temp\\ipykernel_13460\\2019932120.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "my_model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
    "\n",
    "# Instantiate our custom data collator\n",
    "data_collator = DataCollatorForRegression(tokenizer=tokz)\n",
    "\n",
    "# Define Training Arguments (with fp16=False for CPU)\n",
    "# bs = 5\n",
    "# epochs = 4\n",
    "bs = 100\n",
    "epochs = 1\n",
    "lr = 8e-5\n",
    "\n",
    "args = TrainingArguments(\n",
    "    'outputs', \n",
    "    learning_rate=lr, \n",
    "    warmup_ratio=0.1, \n",
    "    lr_scheduler_type='cosine',\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    per_device_train_batch_size=bs, \n",
    "    per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, \n",
    "    weight_decay=0.01, \n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# Pass the custom data_collator to the Trainer\n",
    "trainer = Trainer(\n",
    "    my_model, \n",
    "    args, \n",
    "    train_dataset=tokenized_ds_train, \n",
    "    eval_dataset=tokenized_ds_cv,\n",
    "    tokenizer=tokz,\n",
    "    data_collator=data_collator  # <-- Use our custom collator here\n",
    ")\n",
    "# ----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 17/272 [02:02<30:30,  7.18s/it]\n",
      "100%|██████████| 4/4 [00:44<00:00,  9.31s/it]\n",
      "                                             \n",
      "\u001b[A                                            \n",
      "\n",
      "100%|██████████| 4/4 [00:48<00:00,  9.31s/it]\n",
      "\u001b[A\n",
      "                                             \n",
      "100%|██████████| 4/4 [00:48<00:00, 12.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7904660105705261, 'eval_runtime': 0.7498, 'eval_samples_per_second': 56.014, 'eval_steps_per_second': 1.334, 'epoch': 1.0}\n",
      "{'train_runtime': 48.9708, 'train_samples_per_second': 6.861, 'train_steps_per_second': 0.082, 'train_loss': 0.8590915203094482, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=0.8590915203094482, metrics={'train_runtime': 48.9708, 'train_samples_per_second': 6.861, 'train_steps_per_second': 0.082, 'total_flos': 1964278084080.0, 'train_loss': 0.8590915203094482, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train (Here, fine tune) the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 42/42 [00:00<00:00, 173.77 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 1346.03 examples/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on Test Set: 0.9516604943644433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Report loss for your model using the test set\n",
    "# Use the corrected tokenization function, passing 'tokz'\n",
    "tokenized_ds_test = tokenize_dataset(ds_test, tokz)\n",
    "\n",
    "preds = trainer.predict(tokenized_ds_test).predictions.astype(float)\n",
    "\n",
    "# Using MAE to calculate loss\n",
    "def get_mae(preds, real):\n",
    "    '''\n",
    "    preds, real: array\n",
    "    '''\n",
    "    mae = np.mean(np.abs(preds - real))\n",
    "    return mae\n",
    "\n",
    "real = np.array(tokenized_ds_test['labels'])\n",
    "print(f\"MAE on Test Set: {get_mae(preds, real)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE of my model: 0.9 (Based on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "   Prediction Results on New Examples\n",
      "========================================\n",
      "\n",
      "Input:           'USER_CONTEXT: \"developer\"; URL_ROOT: \"youtube\"'\n",
      "Predicted Score: 0.1749\n",
      "Final Label:     0  (-1: Distractive, 0: Neutral, 1: Productive)\n",
      "----------------------------------------\n",
      "\n",
      "Input:           'USER_CONTEXT: \"marketing\"; URL_ROOT: \"youtube\"'\n",
      "Predicted Score: 0.1754\n",
      "Final Label:     0  (-1: Distractive, 0: Neutral, 1: Productive)\n",
      "----------------------------------------\n",
      "\n",
      "Input:           'USER_CONTEXT: \"sales\"; URL_ROOT: \"linkedin\"'\n",
      "Predicted Score: 0.1747\n",
      "Final Label:     0  (-1: Distractive, 0: Neutral, 1: Productive)\n",
      "----------------------------------------\n",
      "\n",
      "Input:           'USER_CONTEXT: \"developer\"; URL_ROOT: \"linkedin\"'\n",
      "Predicted Score: 0.1746\n",
      "Final Label:     0  (-1: Distractive, 0: Neutral, 1: Productive)\n",
      "----------------------------------------\n",
      "\n",
      "Input:           'USER_CONTEXT: \"human_resource\"; URL_ROOT: \"netflix\"'\n",
      "Predicted Score: 0.1720\n",
      "Final Label:     0  (-1: Distractive, 0: Neutral, 1: Productive)\n",
      "----------------------------------------\n",
      "\n",
      "Input:           'USER_CONTEXT: \"developer\"; URL_ROOT: \"google\"'\n",
      "Predicted Score: 0.1754\n",
      "Final Label:     0  (-1: Distractive, 0: Neutral, 1: Productive)\n",
      "----------------------------------------\n",
      "\n",
      "Input:           'USER_CONTEXT: \"sales\"; URL_ROOT: \"github\"'\n",
      "Predicted Score: 0.1757\n",
      "Final Label:     0  (-1: Distractive, 0: Neutral, 1: Productive)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "my_model.eval()\n",
    "\n",
    "# 1. Define a few new examples to test the model's logic\n",
    "test_examples = [\n",
    "    # --- Test Case 1: Context-dependent site (YouTube) ---\n",
    "    'USER_CONTEXT: \"developer\"; URL_ROOT: \"youtube\"',      # Expected: -1 (Distractive)\n",
    "    'USER_CONTEXT: \"marketing\"; URL_ROOT: \"youtube\"',      # Expected: 1 (Productive)\n",
    "    \n",
    "    # --- Test Case 2: Context-dependent site (LinkedIn) ---\n",
    "    'USER_CONTEXT: \"sales\"; URL_ROOT: \"linkedin\"',         # Expected: 1 (Productive)\n",
    "    'USER_CONTEXT: \"developer\"; URL_ROOT: \"linkedin\"',     # Expected: -1 (Distractive)\n",
    "    \n",
    "    # --- Test Case 3: \"Always Distractive\" site ---\n",
    "    'USER_CONTEXT: \"human_resource\"; URL_ROOT: \"netflix\"', # Expected: -1 (Distractive)\n",
    "    \n",
    "    # --- Test Case 4: \"Always Neutral\" site ---\n",
    "    'USER_CONTEXT: \"developer\"; URL_ROOT: \"google\"',       # Expected: 0 (Neutral)\n",
    "    \n",
    "    # --- Test Case 5: \"Always Productive\" site ---\n",
    "    'USER_CONTEXT: \"sales\"; URL_ROOT: \"github\"',           # Expected: 1 (Productive)\n",
    "]\n",
    "\n",
    "# 2. Tokenize the examples\n",
    "# We send the text to the tokenizer and get PyTorch tensors back\n",
    "inputs = tokz(test_examples, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move tensors to the same device as the model (CPU in your case)\n",
    "device = my_model.device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# 3. Make predictions\n",
    "# We use torch.no_grad() to disable gradient calculations, which saves memory and speeds up inference\n",
    "with torch.no_grad():\n",
    "    outputs = my_model(**inputs)\n",
    "\n",
    "# The model outputs raw logits. For our regression task, this is a single value per input.\n",
    "# We flatten the output and move it back to the CPU to work with numpy/pandas\n",
    "predictions = outputs.logits.flatten().cpu().numpy()\n",
    "\n",
    "\n",
    "# 4. Display the results\n",
    "print(\"=\"*40)\n",
    "print(\"   Prediction Results on New Examples\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for i, example in enumerate(test_examples):\n",
    "    raw_score = predictions[i]\n",
    "    \n",
    "    # Round the raw score to the nearest integer to get the final label\n",
    "    final_label = round(raw_score)\n",
    "    \n",
    "    print(f\"\\nInput:           '{example}'\")\n",
    "    print(f\"Predicted Score: {raw_score:.4f}\")\n",
    "    print(f\"Final Label:     {final_label}  (-1: Distractive, 0: Neutral, 1: Productive)\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if your GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
